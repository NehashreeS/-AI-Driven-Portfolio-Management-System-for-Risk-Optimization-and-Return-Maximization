{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize logging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(_name_)\n",
    "\n",
    "# Step 1: Load the Data\n",
    "def load_data():\n",
    "    logger.info(\"Loading data...\")\n",
    "    fundamentals = pd.read_csv(r\"C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\fundamentals.csv\")\n",
    "    prices_split_adjusted = pd.read_csv(r\"C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\prices-split-adjusted.csv\")\n",
    "    securities = pd.read_csv(r\"C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\securities.csv\")\n",
    "    \n",
    "    # Display the first few rows of the loaded data for verification\n",
    "    logger.info(f\"Fundamentals head:\\n{fundamentals.head()}\")\n",
    "    logger.info(f\"Prices head:\\n{prices_split_adjusted.head()}\")\n",
    "    logger.info(f\"Securities head:\\n{securities.head()}\")\n",
    "    \n",
    "    logger.info(\"Data loaded successfully.\")\n",
    "    return fundamentals, prices_split_adjusted, securities\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "def preprocess_data(fundamentals, prices_split_adjusted, securities):\n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    fundamentals = fundamentals.rename(columns={'Ticker Symbol': 'ticker'})\n",
    "    prices_split_adjusted = prices_split_adjusted.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "    # Ensure fundamentals have unique tickers\n",
    "    fundamentals = fundamentals.drop_duplicates(subset='ticker')\n",
    "\n",
    "    # Merge datasets\n",
    "    data = pd.merge(prices_split_adjusted, fundamentals, on='ticker', how='inner')\n",
    "    data = pd.merge(data, securities[['Ticker symbol', 'GICS Sector']], left_on='ticker', right_on='Ticker symbol')\n",
    "\n",
    "    # Compute returns\n",
    "    data['3M_return'] = data.groupby('ticker')['close'].pct_change(periods=63)\n",
    "    data['6M_return'] = data.groupby('ticker')['close'].pct_change(periods=126)\n",
    "    data['12M_return'] = data.groupby('ticker')['close'].pct_change(periods=252)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Scale selected features\n",
    "    columns_to_scale = ['3M_return', '6M_return', '12M_return']\n",
    "    if 'P/E' in data.columns:\n",
    "        columns_to_scale.append('P/E')\n",
    "    if 'ROE' in data.columns:\n",
    "        columns_to_scale.append('ROE')\n",
    "\n",
    "    scaled_features = scaler.fit_transform(data[columns_to_scale])\n",
    "    scaled_column_names = [col + '_scaled' for col in columns_to_scale]\n",
    "    data[scaled_column_names] = scaled_features\n",
    "\n",
    "    logger.info(f\"Data preprocessing completed. Total records: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "# Step 3: Label Creation\n",
    "def assign_label(row):\n",
    "    if row['3M_return'] > 0.2:\n",
    "        return 'StrongBuy'\n",
    "    elif row['3M_return'] > 0.1:\n",
    "        return 'Buy'\n",
    "    elif row['3M_return'] < -0.2:\n",
    "        return 'StrongSell'\n",
    "    elif row['3M_return'] < -0.1:\n",
    "        return 'Sell'\n",
    "    else:\n",
    "        return 'Hold'\n",
    "\n",
    "def label_data(data):\n",
    "    logger.info(\"Assigning labels...\")\n",
    "    data['label'] = data.apply(assign_label, axis=1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['label_encoded'] = label_encoder.fit_transform(data['label'])\n",
    "    logger.info(f\"Labels assigned successfully. Classes: {label_encoder.classes_}\")\n",
    "    return data, label_encoder\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "def split_data(data):\n",
    "    feature_columns = ['3M_return_scaled', '6M_return_scaled', '12M_return_scaled']\n",
    "    if 'P/E_scaled' in data.columns:\n",
    "        feature_columns.append('P/E_scaled')\n",
    "    if 'ROE_scaled' in data.columns:\n",
    "        feature_columns.append('ROE_scaled')\n",
    "\n",
    "    X = data[feature_columns]\n",
    "    y = data['label_encoded']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save the splitted data to CSV files\n",
    "    X_train.to_csv(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\X_train.csv', index=False)\n",
    "    X_test.to_csv(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\X_test.csv', index=False)\n",
    "    y_train.to_csv(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\y_train.csv', index=False)\n",
    "    y_test.to_csv(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\y_test.csv', index=False)\n",
    "\n",
    "    logger.info(f\"Training data size: {len(X_train)}\")\n",
    "    logger.info(f\"Test data size: {len(X_test)}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 5: Feature Selection using Recursive Feature Elimination (RFE)\n",
    "def feature_selection(X, y):\n",
    "    logger.info(\"Selecting features using Recursive Feature Elimination (RFE)...\")\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Set n_features_to_select to 3 or fewer based on the available features\n",
    "    rfe = RFE(model, n_features_to_select=min(3, X.shape[1]))  # Select top 3 or fewer features\n",
    "    X_selected = rfe.fit_transform(X, y)\n",
    "    \n",
    "    selected_features = [col for col, support in zip(X.columns, rfe.support_) if support]\n",
    "    logger.info(f\"Selected Features: {selected_features}\")\n",
    "    \n",
    "    return X_selected, selected_features\n",
    "\n",
    "# Step 6: Handle Class Imbalance with SMOTE\n",
    "def handle_class_imbalance(X_train, y_train):\n",
    "    logger.info(\"Handling class imbalance using SMOTE...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    logger.info(f\"After SMOTE: X_train shape: {X_train_smote.shape}, y_train shape: {y_train_smote.shape}\")\n",
    "    return X_train_smote, y_train_smote\n",
    "\n",
    "# Step 7: Train Fusion Model with VotingClassifier\n",
    "def train_fusion_model(X_train, y_train):\n",
    "    logger.info(\"Training the fusion model...\")\n",
    "\n",
    "    # Define base models\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=50, max_depth=6, learning_rate=0.001,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42\n",
    "    )\n",
    "    model2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model3 = LogisticRegression(max_iter=500, random_state=42, C=1)\n",
    "\n",
    "    # Create VotingClassifier\n",
    "    fusion_model = VotingClassifier(\n",
    "        estimators=[('xgb', model1), ('rf', model2), ('lr', model3)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    fusion_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Step 8: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    logger.info(\"Evaluating the model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    logger.info(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    logger.info(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Visualize Confusion Matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    return accuracy, cm\n",
    "\n",
    "# Step 9: Save the Model\n",
    "def save_model(model, destination_path):\n",
    "    logger.info(f\"Saving the model to {destination_path}...\")\n",
    "    joblib.dump(model, destination_path)\n",
    "    logger.info(\"Model saved successfully.\")\n",
    "\n",
    "# Step 10: Plotting Graphs\n",
    "def plot_graphs(data):\n",
    "    logger.info(\"Plotting graphs...\")\n",
    "\n",
    "    # Scatter Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=data['3M_return'], y=data['6M_return'], hue=data['label'])\n",
    "    plt.title(\"3M vs 6M Returns\")\n",
    "    plt.savefig(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\scatter_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Box Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=data['label'], y=data['3M_return'])\n",
    "    plt.title(\"Box Plot of 3M Return by Label\")\n",
    "    plt.savefig(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\box_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Distribution of Returns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data['3M_return'], kde=True)\n",
    "    plt.title(\"Distribution of 3M Return\")\n",
    "    plt.savefig(r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\distribution_3M_return.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main Execution\n",
    "if _name_ == \"_main_\":\n",
    "    # Load Data\n",
    "    fundamentals, prices_split_adjusted, securities = load_data()\n",
    "\n",
    "    # Preprocess Data\n",
    "    data = preprocess_data(fundamentals, prices_split_adjusted, securities)\n",
    "\n",
    "    # Label the data\n",
    "    data, label_encoder = label_data(data)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = split_data(data)\n",
    "\n",
    "    # Feature selection\n",
    "    X_train_selected, selected_features = feature_selection(X_train, y_train)\n",
    "\n",
    "    # Handle class imbalance\n",
    "    X_train_smote, y_train_smote = handle_class_imbalance(X_train_selected, y_train)\n",
    "\n",
    "    # Train the fusion model\n",
    "    fusion_model = train_fusion_model(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, cm = evaluate_model(fusion_model, X_test, y_test, label_encoder)\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = r'C:\\\\Users\\\\aksha\\\\Desktop\\\\nyse\\\\fusion_model.pkl'\n",
    "    save_model(fusion_model, model_save_path)\n",
    "\n",
    "    # Plot graphs\n",
    "    plot_graphs(data)\n",
    "\n",
    "    logger.info(\"Process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
